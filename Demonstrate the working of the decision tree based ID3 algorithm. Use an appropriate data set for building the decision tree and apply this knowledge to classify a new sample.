import pandas as pd
import math
from collections import Counter

def entropy(data, target_column):
    """Calculate entropy of a dataset"""
    counts = Counter(data[target_column])
    entropy = 0.0
    for count in counts.values():
        probability = count / len(data[target_column])
        entropy -= probability * math.log2(probability)
    return entropy

def information_gain(data, feature, target_column):
    """Calculate information gain for a feature"""
    total_entropy = entropy(data, target_column)
    values = data[feature].unique()
    
    weighted_entropy = 0.0
    for value in values:
        subset = data[data[feature] == value]
        weight = len(subset) / len(data)
        weighted_entropy += weight * entropy(subset, target_column)
    
    return total_entropy - weighted_entropy

def id3(data, original_data, features, target_column, parent_node_class=None):
    """ID3 algorithm to build decision tree"""
    # Base cases
    if len(data[target_column].unique()) <= 1:
        return data[target_column].iloc[0]
    elif len(features) == 0:
        return Counter(data[target_column]).most_common(1)[0][0]
    elif len(data) == 0:
        return Counter(original_data[target_column]).most_common(1)[0][0]
    else:
        # Choose best feature to split on
        best_feature = max(features, key=lambda f: information_gain(data, f, target_column))
        tree = {best_feature: {}}
        
        # Remove best feature from features
        remaining_features = [f for f in features if f != best_feature]
        
        # Build subtree for each value of best feature
        for value in data[best_feature].unique():
            subset = data[data[best_feature] == value]
            subtree = id3(subset, original_data, remaining_features, target_column, parent_node_class)
            tree[best_feature][value] = subtree
        
        return tree

def classify(tree, sample):
    """Classify a sample using the decision tree"""
    if not isinstance(tree, dict):
        return tree  # Leaf node, return class
    
    feature = next(iter(tree))
    value = sample.get(feature)
    
    if value not in tree[feature]:
        return None  # Unknown value
    
    return classify(tree[feature][value], sample)

def print_tree(tree, indent=0):
    """Print the decision tree"""
    if not isinstance(tree, dict):
        print(" " * indent + "→ " + str(tree))
        return
    
    for key, value in tree.items():
        print(" " * indent + str(key))
        if isinstance(value, dict):
            for k, v in value.items():
                print(" " * (indent + 4) + str(k))
                print_tree(v, indent + 8)
        else:
            print(" " * (indent + 4) + "→ " + str(value))

# Example dataset (Play Tennis)
data = {
    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 
                'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],
    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 
                    'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],
    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 
                 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],
    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 
             'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],
    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 
                   'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
}

df = pd.DataFrame(data)
features = ['Outlook', 'Temperature', 'Humidity', 'Wind']
target = 'PlayTennis'

# Build decision tree
decision_tree = id3(df, df, features, target)

print("Decision Tree:")
print_tree(decision_tree)

# Classify new samples
new_samples = [
    {'Outlook': 'Sunny', 'Temperature': 'Cool', 'Humidity': 'High', 'Wind': 'Strong'},
    {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'Normal', 'Wind': 'Weak'},
    {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'Normal', 'Wind': 'Weak'}
]

print("\nClassification Results:")
for sample in new_samples:
    classification = classify(decision_tree, sample)
    print(f"Sample: {sample} → Prediction: {classification}")
